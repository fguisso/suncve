{"cveId": "CVE-2024-12704", "cwe": ["CWE-755", "CWE-755", "CWE-755"], "cvss": [{"baseScore": "7.5", "version": "3.0", "Attack Vector": "None", "Attack Complexity": "Low", "Privileges Required": "None", "User Interaction": "None", "Scope": "Unchaged", "Confidentiality Impact": "None", "Integrity Impact": "None", "Availability Impact": "High"}], "references": ["https://huntr.com/bounties/a0b638fd-21c6-4ba7-b381-6ab98472a02a", "https://github.com/run-llama/llama_index/commit/d1ecfb77578d089cbe66728f18f635c09aa32a05"], "description": ["A vulnerability in the LangChainLLM class of the run-llama/llama_index repository, version v0.12.5, allows for a Denial of Service (DoS) attack. The stream_complete method executes the llm using a thread and retrieves the result via the get_response_gen method of the StreamingGeneratorCallbackHandler class. If the thread terminates abnormally before the _llm.predict is executed, there is no exception handling for this case, leading to an infinite loop in the get_response_gen function. This can be triggered by providing an input of an incorrect type, causing the thread to terminate and the process to continue running indefinitely."], "published": "2025-03-20T10:15:29.383", "state": "PUBLIC", "vendorName": ["run-llama"], "productName": ["run-llama/llama_index"], "github": {"advisories": [], "commits": ["https://github.com/run-llama/llama_index/commit/d1ecfb77578d089cbe66728f18f635c09aa32a05"], "pocAdvisorie": null, "repo": "https://github.com/run-llama/llama_index/", "info": {"exist": true, "topics": ["agents", "application", "data", "fine-tuning", "framework", "llamaindex", "llm", "multi-agents", "rag", "vector-database"], "langs": {"Python": 12955767, "Jupyter Notebook": 11305578, "Makefile": 430053, "JavaScript": 85982, "Starlark": 78265, "Tree-sitter Query": 17376, "Dockerfile": 5734, "Shell": 4557}, "avatar": "https://avatars.githubusercontent.com/u/130722866?v=4", "stargazers": 40717, "language": "Python"}}, "pocList": []}